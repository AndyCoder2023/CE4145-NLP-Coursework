{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNI1guiAyKpl4708t+4Pf/H",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AndyCoder2023/CE4145-NLP-Coursework/blob/main/CE4145_Coursework.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "“Generative AI was used to support completion of this assessment. If used: The GenAI tool [Gemini] was used for the purpose of [ideation/editing]. Where used for the purpose of supporting development,\n",
        "comments have been provided against relevant cells.\n",
        "\n",
        "Prompt [1]:\n",
        "Please explain this error:\n",
        "The error FileNotFoundError: hf://datasets/Pablinho/movies-dataset/9000plus.csv not found in cell 8Ypu2LZD3FOP means that the code is looking for a file at the specified Hugging Face dataset path, but it cannot find it.\n",
        "Looking at the available variables, it seems the movie data is already loaded into the movie_df DataFrame. I will remove the redundant data loading code from cell 8Ypu2LZD3FOP and use the existing movie_df to create the x and y variables for the NLP pipeline.\n",
        "It seems there are missing values in the data that are causing the ValueError: Input contains NaN. I will add code to remove rows with missing values in the 'Overview' or 'Genre' columns before processing the data.\n",
        "\n",
        "Prompt [2]\n",
        "\"ValueError: [E1041] Expected a string, Doc, or bytes as input, but got: <class 'numpy.ndarray\": The error in cell Ri_Mi50-KjQh is a ValueError because the spaCy nlp function expects a single document (a string, Doc, or bytes), but it received a NumPy array containing all the overviews. To fix this, I will iterate through each overview in the 'Overview' column of the df DataFrame and apply the nlp function to each one individually. I will also add code to skip any missing values in the 'Overview' column to prevent further errors.\n",
        "\n",
        "Prompt [3]\n",
        "I see the confusion! You're mixing up KNN (K-Nearest Neighbors) and MLP (Multi-Layer Perceptron). They're completely different algorithms:\n",
        "\n",
        "MLP = Neural Network (what you've defined with Keras)\n",
        "KNN = A scikit-learn classifier (different library, different algorithm)\n",
        "\n",
        "Here's the corrected code:\n",
        "pythonfrom tensorflow import keras\n",
        "from keras import layers\n",
        "from keras import activations\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.neighbors import KNeighborsClassifier  # Import KNN\n",
        "\n",
        "# Encode labels\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "\n",
        "# Split data\n",
        "x_train, x_test, y_train, y_test = train_test_split(\n",
        "    x, y_encoded, test_size=0.33, random_state=1337\n",
        ")\n",
        "\n",
        "# Vectorize text data\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "x_train_vec = tfidf_vectorizer.fit_transform(x_train)\n",
        "x_test_vec = tfidf_vectorizer.transform(x_test)\n",
        "\n",
        "num_classes = len(label_encoder.classes_)\n",
        "\n",
        "# ===== KNN MODEL (scikit-learn) =====\n",
        "knn = KNeighborsClassifier(n_neighbors=5)  # Create KNN classifier\n",
        "knn.fit(x_train_vec, y_train)  # KNN uses original labels, not one-hot\n",
        "knn_predictions = knn.predict(x_test_vec)\n",
        "knn_accuracy = knn.score(x_test_vec, y_test)\n",
        "print(f\"KNN Accuracy: {knn_accuracy}\")\n",
        "\n",
        "# ===== MLP MODEL (Keras/TensorFlow) =====\n",
        "# Convert to one-hot for neural network\n",
        "y_traink = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_testk = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "input_dim = x_train_vec.shape[1]\n",
        "\n",
        "# Define MLP\n",
        "mlp = keras.Sequential([\n",
        "    keras.Input(shape=(input_dim,)),\n",
        "    keras.layers.Dense(64, activation='relu'),\n",
        "    keras.layers.Dense(num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "mlp.compile(\n",
        "    loss=\"categorical_crossentropy\",\n",
        "    optimizer=\"sgd\",\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "# Train MLP (convert sparse to dense)\n",
        "mlp.fit(\n",
        "    x_train_vec.todense(),\n",
        "    y_traink,\n",
        "    batch_size=64,\n",
        "    epochs=10,\n",
        "    validation_split=0.2  # Optional: adds validation during training\n",
        ")\n",
        "\n",
        "# Evaluate MLP\n",
        "mlp_loss, mlp_accuracy = mlp.evaluate(x_test_vec.todense(), y_testk)\n",
        "print(f\"MLP Accuracy: {mlp_accuracy}\")\n",
        "Key differences:\n",
        "\n",
        "KNN: Uses fit() with original labels (y_train), not one-hot encoded\n",
        "MLP: Uses fit() with one-hot encoded labels (y_traink)\n",
        "KNN: Simple scikit-learn classifier\n",
        "MLP: Neural network that needs compilation and training epochs\n",
        "\n",
        "Prompt [4]\n",
        "Please explain this error:\n",
        "\n",
        "It looks like the error is due to the input shape of the data being fed into the MLP model. The model is expecting numerical feature vectors, but it's receiving raw text data. I will add a vectorization step to convert the text into numerical features and adjust the MLP model to accept the vectorized input.\n",
        "\n",
        "I have fixed the error by vectorizing the text data before feeding it into the MLP model and adjusted the model to accept the vectorized input. The model is now training.\n",
        "\n",
        "Next, you can evaluate the trained MLP model using the test set and compare its performance with the KNN model you trained earlier. You can then analyze the results to determine which approach is more effective for your task.\n",
        "\n",
        "\n",
        "i want knn algorithm in this data\n",
        "You already have the KNN algorithm set up in cell ku4DdlyY8T59 using the 'Title' column. Would you like to apply the KNN algorithm to the 'Overview' column as well, similar to how you are using it for the MLP model? If so, I can generate the code for that.\n",
        "\n",
        "Please explain this error:\n",
        "\n",
        "It seems the error is because the boxplot function is expecting a list of data series to plot, but it's receiving a single list. I'll adjust the code to correctly plot the two accuracy scores as separate box plots.\n",
        "\n",
        "I apologize, it seems I made a mistake in the previous fix. To plot two separate box plots with their respective labels, I need to pass the data to boxplot as a list of lists, where each inner list contains the data for a single box plot. I will correct the code to do this.\n",
        "\n",
        "The box plot comparing the performance of the KNN and MLP algorithms has been generated successfully.\n",
        "\n",
        "Now you can analyze the plot to compare the accuracy of the two models. What would you like to do next? Perhaps you'd like to interpret the results of the box plot, or explore ways to improve the performance of either model?\n",
        "\n",
        "Prompt[4]\n",
        "Please explain this error:\n",
        "\n",
        "The error is a TypeError because you're trying to use the knn and mlp model objects as list indices, which is not allowed. To fix this, I will set the x variable to be the labels 'knn' and 'mlp', and the y variable to be model_comp containing the accuracy values. This will correctly prepare the data for the bar plot.\n",
        "\n",
        "The bar plot comparing the performance of the KNN and MLP algorithms has been generated successfully. You can now analyze this plot to compare the accuracy of the two models.\n",
        "\n"
      ],
      "metadata": {
        "id": "juUbtdmWCO8v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset\n",
        "\n",
        "The dataset has 9 columns with 9.84k rows of movie data. The task that is to be performed for the dataset will consist of two different pipelines which will compare the best strategy for the given task to be performed on the movie dataset. The main problem-solving task at hand will be to find out what nlp algorithm is best for classification using the statistics available in the dataset based on the movie genres. An NLP system is required for this objective to find and narrow down the dataset to pinpoint the algorithm that works the best. By implementing an NLP system there can be strong analysis on the dataset to find the correct criteria. The data will be pre-processed to allow data to not be missed or inconsistent data retrieval. Dataset will also use punkt and stopword modules for better data pre-processing to get more accurate readings in the algorithms. Columns used for the data classification will be the title and genre columns.\n",
        "\n",
        "Dataset link: https://huggingface.co/datasets/Pablinho/movies-dataset."
      ],
      "metadata": {
        "id": "gibHCW4NBSdD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files #import the files package from google.colab framework to be able to upload files\n",
        "\n",
        "from sklearn.pipeline import Pipeline #let's import the pipeline functionality\n",
        "from sklearn.feature_extraction.text import CountVectorizer #and we will import a simple pre-processing method\n",
        "from sklearn.feature_extraction.text import TfidfTransformer #and a representation learner\n",
        "from sklearn.neighbors import KNeighborsClassifier #and a simple classifier model\n",
        "from sklearn.model_selection import StratifiedKFold #cross fold is sometimes called k-fold. Calling the stratified version ensures that classes have equal representation across folds\n",
        "from sklearn.metrics import accuracy_score #import an accuracy metric to tell us how well the model is doing\n",
        "\n",
        "import numpy as np\n",
        "np.random.seed(1337)\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"hf://datasets/Pablinho/movies-dataset/9000plus.csv\")\n"
      ],
      "metadata": {
        "id": "8Ypu2LZD3FOP",
        "outputId": "a786f508-86e5-4bee-fc1e-c3b0a189886c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "delimiter = ','\n",
        "first_genre_cell = df['Genre'].iloc[0]\n",
        "print(first_genre_cell)\n",
        "target_genre = first_genre_cell.split(delimiter)[0].strip()\n",
        "print(target_genre)\n",
        "target_genre = first_genre_cell.split(delimiter)[0].strip()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xbkzvID1zTGF",
        "outputId": "382672d7-b040-45eb-9a64-ea76927a249e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Action, Adventure, Science Fiction\n",
            "Action\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.dropna(subset=['Genre'])"
      ],
      "metadata": {
        "id": "RezhahTR-K-W"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating new Split_Genre column to get the first value of each cell in genre column\n",
        "df['Split_Genre'] = df['Genre'].str.split(', ', expand=True)[0]\n",
        "# Create the new column, filling all rows with NaN initially\n",
        "df['Split_Genre'][2]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "b3Kt68Pp3QVS",
        "outputId": "ab34cb00-720a-44e2-e12c-92efa80d8624"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Thriller'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Used ai to fix gathering dataset not being found. [1]\n",
        "\n",
        "#target_genre = 'Action', 'Adventure', 'Animation', 'Comedy', 'Crime'\n",
        "#df['is_Target_Genre'] = (df['Genre'] == target_genre).astype(int)\n",
        "#print(df.head(5))\n",
        "\n",
        "x = df['Overview'].astype(str).values\n",
        "y = df['Split_Genre'].values\n",
        "\n",
        "text_clf = Pipeline([ #the pipeline object allows us to organise a series of functions which will be applied to our text data as though they were a single function\n",
        "  ('prep', CountVectorizer(max_features=500)), #we will use a simple count vectorizer for our pre-processing (which cheats a little by combining numerous pre-processing steps)\n",
        "  ('rep', TfidfTransformer()), #and a representation learning method using tf-idf\n",
        "  ('mod', KNeighborsClassifier()), #and a simple kNN classifier\n",
        "  ])\n",
        "\n",
        "acc_score = [] #create a list to store the accuracy values\n",
        "\n",
        "kf = StratifiedKFold(n_splits=5) #we instantiate the kfold instance, and set the number of folds to 5\n",
        "for train, test in kf.split(x,y): #we use a for loop to iterate through each fold using the train and test indexes from the dataset\n",
        "\n",
        "  x_train, x_test, y_train, y_test = x[train], x[test], y[train], y[test] #things can get a bit weird when inputting indexes to functions, so lets save them as variables\n",
        "  #print(train)\n",
        "  #print(test) #this will print the train and test indexes respectively, if you want to be sure they do not overlap\n",
        "\n",
        "  text_clf.fit(x_train, y_train) #we then only fit the training data (note that we oapply the text_clf pipeline object, rather than having to go through each function separately)\n",
        "  predictions = text_clf.predict(x_test) #and can predict on the test data (similar to above, we can predict using the pipeline directly)\n",
        "  acc = accuracy_score(predictions, y_test) #we use the accuracy score we imported to give an idea how well the model is doing\n",
        "  acc_score.append(acc) #we can append it to our list\n",
        "\n",
        "print(\"Accuracy:\", np.mean(acc_score))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ku4DdlyY8T59",
        "outputId": "0b2f20ef-46e2-406f-8a7d-100ffbeab8dd"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.2299019722043182\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(x_train[0]) #first let's double check our input data is just a simple sentence"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ONJfHmHxPzrH",
        "outputId": "59dfb48f-e1ca-4be2-b344-e529a2e36c2a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Peter Parker is unmasked and no longer able to separate his normal life from the high-stakes of being a super-hero. When he asks for help from Doctor Strange the stakes become even more dangerous, forcing him to discover what it truly means to be Spider-Man.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk #import the natural language toolkit\n",
        "\n",
        "nltk.download('punkt') #download the package in nltk which supports tokenization\n",
        "nltk.download('stopwords') #download the nltk package for stopwords\n",
        "\n",
        "from nltk.tokenize import word_tokenize #import the tokenize package\n",
        "from nltk.corpus import stopwords #import the package from the corpus\n",
        "from nltk.stem.snowball import SnowballStemmer #import the snowball stemmer (also known as Porter2)\n",
        "\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "class pre_process(BaseEstimator, TransformerMixin):\n",
        "\n",
        "    def __init__(self):\n",
        "      return None #we do not need any parameters to instantiate this class\n",
        "\n",
        "    def fit(self, X, y=None): #both fit and transform expect the data instances and labels to be called - we do not use the labels, so set y=None\n",
        "        return self #as explained above, we will not use the fit method\n",
        "\n",
        "    def transform(self, X, y=None):\n",
        "      prep_text = []\n",
        "      # tokenize genre and keep first token **\n",
        "      for x in X: #for each sentence in the whole dataset\n",
        "            token_text = word_tokenize(x) #tokenize the document\n",
        "\n",
        "            normd_text = [token.lower() for token in token_text if token.isalpha()] #list compression to apply some simple cleaning (lower case casting and punctuation removal) to tokenized terms\n",
        "\n",
        "            swr_text = [token for token in normd_text if token not in stopwords.words('english')] #list compression to remove any stopwords from our list\n",
        "\n",
        "            stemmer = SnowballStemmer(\"english\") #specify we are using the English stemming rules, as other languages are present in toolkit\n",
        "            prep_text += [[stemmer.stem(word) for word in swr_text]] #list compression for applying the stemmer\n",
        "\n",
        "      prep_sentences = [\" \".join(sentence) for sentence in prep_text] #we join the sentences back together to ensure compatibility with CountVec, which is doing some of it's own prep\n",
        "      return prep_sentences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ubufY8rF6i1t",
        "outputId": "fd04faee-5125-4534-816f-7959a4d5c8cf"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['Split_Genre'][0]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "ezdCecKE2U5J",
        "outputId": "e8776dcb-792e-4f51-b91b-fe60c55ad64c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Action'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "unique_genres = df['Split_Genre'].nunique()\n",
        "print(\"Unique genre count: \" + str(unique_genres))\n",
        "genreCount = df['Split_Genre'].count()\n",
        "print(\"Genre cell count: \" + str(genreCount))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9kiRp9caqlpQ",
        "outputId": "05add457-bb00-4211-ed94-e55606d6af22"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unique genre count: 19\n",
            "Genre cell count: 9826\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Representation Learning  \n",
        "\n",
        "The representation learning method used will be the Word-2-Vector (Word2Vec). Word2Vec known as a technique which is in Natural language processing. The technique permits words to have representation as vectors which are in a continuous vector space. The representation learned for every word will be \"predicting a target term using its context (CBOW)\". ​Continuous Bag of Words (CBOW) model​ will be used to forecast a term that is ​given​ words that is located in a window. Input layer will have the context words while output layer will have a current word. Hidden layer has dimensions that will show a current word that is located on the output layer (Kadam, 2025) (Suri, 2022). Another training architecture in Word2Vec other than continuous bag of words is ​Skip-Gram.​ ​Skip Gram​ works oppositely to cbow by taking the input word and then the model predicts what words the input word will be next to (Suri, 2022).\n",
        "\n",
        "Sumedh, K. (2025) ​Word Embedding using Word2Vec. Available at:​ ​https://www.geeksforgeeks.org/python​ /python- ​word-embedding-using-​ ​word2vec/ (Accessed: 22​ October 2025).  \n",
        "\n",
        "Manan S. (​2022) A Dummy's Guide to Word2Vec. Available at:​ ​https://medium.com/@manansuri/a-dummys-guide-to-word2vec-456444f3c673​ (Accessed: 01 November 2025)."
      ],
      "metadata": {
        "id": "I4fSC1u8Ebqu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim"
      ],
      "metadata": {
        "id": "VElZcAX79t9i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c63b7cf-9308-444a-d791-7f1644d4c70a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gensim\n",
            "  Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.16.3)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.5.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim) (2.0.1)\n",
            "Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (27.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.9/27.9 MB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: gensim\n",
            "Successfully installed gensim-4.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec #first we import word2vec, then a few familiar imports"
      ],
      "metadata": {
        "id": "Ag4oC9089X-H"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assigning string values\n",
        "texts = df['Overview'].astype(str).values"
      ],
      "metadata": {
        "id": "aWhhjsxzanou"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "wX4O7e_OXDQ7"
      },
      "outputs": [],
      "source": [
        "sentences = [line.split() for line in texts]\n",
        "\n",
        "w2v = Word2Vec(sentences, window=5, workers=4)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(sentences[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FLbuQGZSMjCk",
        "outputId": "3258ef0f-6e26-4bd6-81a6-33a417d09ca6"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Peter', 'Parker', 'is', 'unmasked', 'and', 'no', 'longer', 'able', 'to', 'separate', 'his', 'normal', 'life', 'from', 'the', 'high-stakes', 'of', 'being', 'a', 'super-hero.', 'When', 'he', 'asks', 'for', 'help', 'from', 'Doctor', 'Strange', 'the', 'stakes', 'become', 'even', 'more', 'dangerous,', 'forcing', 'him', 'to', 'discover', 'what', 'it', 'truly', 'means', 'to', 'be', 'Spider-Man.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Algorithms"
      ],
      "metadata": {
        "id": "b34dhQrEIoJM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#MLP  \n",
        "\n",
        "Multi-Layer Perceptron (MLP) has the capability to learn a non - linear function that is used to separate data. All the layers in a multi-layer perceptron are closely connected. Multiple hidden layers are also possible. Multilayer perceptron's have an input layer, hidden layer, and an output layer. There is also a requirement of a numerical vector in the input layer. The number of indexes in a vector will be the size of input layer. Hidden layer is used to connect input and output layer by using weights & biases. There needs to be an activation function in a hidden layer, but it is usually different on each hidden layer. The output layer is dependent on the final activation function but normally softmax is used in classification. The output layer consists of a vector of probabilities that have the same length of the number of classes.  \n",
        "\n",
        "#KNN  \n",
        "\n",
        "The Knn is a supervised machine learning algorithm which can be deployed to provide the task of classification and the algorithm can been used for regression work as well(Kartik, 2025). KNN algorithm works by grabbing the \"k\" nearest data points which are provided by a given input which creates predication from the majority class. KNN is known as a non-parametric & instance learning method because it doesn't have assumptions for majority class. (Kartik, 2025)  \n",
        "\n",
        "K - nearest Neighbour(kNN) consists of series of steps which are - inputting a new instance, compute similarity that is between a new instance and every historical instance, Historical instances are ordered to be based on their similarity, Class labels are voted by using top k neighbours, Finally the label that has the most votes is printed out. Instances are ranked by similarity to query after the similarity calculation.  \n",
        "\n",
        "Kartik. (2025) *K-Nearest Neighbor(KNN) Algorithm*. Available at: https://www.bbc.co.uk/news/science-environment-68406880 (Accessed: 21 November 2025)."
      ],
      "metadata": {
        "id": "IWbMMkS1mrUE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Used AI to help with debugging errors in the algorithms\n",
        "from tensorflow import keras\n",
        "from keras import layers\n",
        "from keras import activations\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.neighbors import KNeighborsClassifier  # Import KNN\n",
        "\n",
        "# Encode labels\n",
        "label_encoder = LabelEncoder()\n",
        "y_encoded = label_encoder.fit_transform(y)\n",
        "\n",
        "# Split data\n",
        "x_train, x_test, y_train, y_test = train_test_split(\n",
        "    x, y_encoded, test_size=0.33, random_state=1337\n",
        ")\n",
        "\n",
        "# Vectorize text data\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "x_train_vec = tfidf_vectorizer.fit_transform(x_train)\n",
        "x_test_vec = tfidf_vectorizer.transform(x_test)\n",
        "\n",
        "num_classes = len(label_encoder.classes_)\n",
        "\n",
        "# ===== KNN MODEL (scikit-learn) =====\n",
        "knn = KNeighborsClassifier(n_neighbors=5)  # Create KNN classifier\n",
        "knn.fit(x_train_vec, y_train)  # KNN uses original labels, not one-hot\n",
        "knn_predictions = knn.predict(x_test_vec)\n",
        "knn_accuracy = knn.score(x_test_vec, y_test)\n",
        "print(f\"KNN Accuracy: {knn_accuracy}\")\n",
        "\n",
        "# ===== MLP MODEL (Keras/TensorFlow) =====\n",
        "# Convert to one-hot for neural network\n",
        "y_traink = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_testk = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "input_dim = x_train_vec.shape[1]\n",
        "\n",
        "# Define MLP\n",
        "mlp = keras.Sequential([\n",
        "    keras.Input(shape=(input_dim,)),\n",
        "    keras.layers.Dense(64, activation='relu'),\n",
        "    keras.layers.Dense(num_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "mlp.compile(\n",
        "    loss=\"categorical_crossentropy\",\n",
        "    optimizer=\"sgd\",\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "# Train MLP (convert sparse to dense)\n",
        "mlp.fit(\n",
        "    x_train_vec.todense(),\n",
        "    y_traink,\n",
        "    batch_size=64,\n",
        "    epochs=10,\n",
        "    validation_split=0.2  # Optional: adds validation during training\n",
        ")\n",
        "\n",
        "# Evaluate MLP\n",
        "mlp_loss, mlp_accuracy = mlp.evaluate(x_test_vec.todense(), y_testk)\n",
        "print(f\"MLP Accuracy: {mlp_accuracy}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SHrGwv65fnfc",
        "outputId": "4ad850d1-b7aa-4996-fde2-514e5c6fee8c"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "KNN Accuracy: 0.30403946962688866\n",
            "Epoch 1/10\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 36ms/step - accuracy: 0.1554 - loss: 2.9252 - val_accuracy: 0.1860 - val_loss: 2.8631\n",
            "Epoch 2/10\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 34ms/step - accuracy: 0.1784 - loss: 2.8470 - val_accuracy: 0.1838 - val_loss: 2.7870\n",
            "Epoch 3/10\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 32ms/step - accuracy: 0.1867 - loss: 2.7716 - val_accuracy: 0.1822 - val_loss: 2.7203\n",
            "Epoch 4/10\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 27ms/step - accuracy: 0.1856 - loss: 2.7131 - val_accuracy: 0.1913 - val_loss: 2.6632\n",
            "Epoch 5/10\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - accuracy: 0.1910 - loss: 2.6567 - val_accuracy: 0.1921 - val_loss: 2.6163\n",
            "Epoch 6/10\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.1901 - loss: 2.6089 - val_accuracy: 0.1921 - val_loss: 2.5797\n",
            "Epoch 7/10\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.1799 - loss: 2.5817 - val_accuracy: 0.1921 - val_loss: 2.5523\n",
            "Epoch 8/10\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.1890 - loss: 2.5523 - val_accuracy: 0.1921 - val_loss: 2.5324\n",
            "Epoch 9/10\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.1963 - loss: 2.5201 - val_accuracy: 0.1921 - val_loss: 2.5178\n",
            "Epoch 10/10\n",
            "\u001b[1m83/83\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.1956 - loss: 2.5200 - val_accuracy: 0.1921 - val_loss: 2.5070\n",
            "\u001b[1m102/102\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.1577 - loss: 2.5517\n",
            "MLP Accuracy: 0.16836263239383698\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation\n",
        "\n",
        "For the evaluation there is a clear more precise accuracy with the knn alogorithm over the mlp algorithm. I believe knn is performing better due to better pre processing and is more appropriate for the task at hand offering better analysis. Overall knn is more accurate than mlp for this comparison."
      ],
      "metadata": {
        "id": "TYIwd6xdI8hp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# AI said i was using knn and mlp model lists as list indices so changed x variables to be knn and ml[]\n",
        "# AI also suggested to change y variable to be model_comp\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "model_comp = []\n",
        "model_comp.append(knn_accuracy)\n",
        "model_comp.append(mlp_accuracy)\n",
        "\n",
        "x = ['knn', 'mlp'] # Labels for the bars\n",
        "y = model_comp    # The accuracy values\n",
        "\n",
        "plt.bar(x, y)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "5u2QJ2fhHKMw",
        "outputId": "57c0b3dd-4b19-45f1-bccc-aaae6e2bcafb"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}