{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOQ4LkZHr4BSWwFC1lS0MA/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AndyCoder2023/CE4145-NLP-Coursework/blob/main/CE4145_Coursework.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "“Generative AI was used to support completion of this assessment. If used: The GenAI tool [Gemini] was used for the purpose of [ideation/editing]. Where used for the purpose of supporting development,\n",
        "comments have been provided against relevant cells.\n",
        "\n",
        "Prompt [1]:\n",
        "Please explain this error:\n",
        "The error FileNotFoundError: hf://datasets/Pablinho/movies-dataset/9000plus.csv not found in cell 8Ypu2LZD3FOP means that the code is looking for a file at the specified Hugging Face dataset path, but it cannot find it.\n",
        "Looking at the available variables, it seems the movie data is already loaded into the movie_df DataFrame. I will remove the redundant data loading code from cell 8Ypu2LZD3FOP and use the existing movie_df to create the x and y variables for the NLP pipeline.\n",
        "It seems there are missing values in the data that are causing the ValueError: Input contains NaN. I will add code to remove rows with missing values in the 'Overview' or 'Genre' columns before processing the data.\n",
        "\n",
        "Prompt [2]\n",
        "\"ValueError: [E1041] Expected a string, Doc, or bytes as input, but got: <class 'numpy.ndarray\": The error in cell Ri_Mi50-KjQh is a ValueError because the spaCy nlp function expects a single document (a string, Doc, or bytes), but it received a NumPy array containing all the overviews. To fix this, I will iterate through each overview in the 'Overview' column of the df DataFrame and apply the nlp function to each one individually. I will also add code to skip any missing values in the 'Overview' column to prevent further errors."
      ],
      "metadata": {
        "id": "juUbtdmWCO8v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Describe the dataset and the task to be performed**\n",
        "\n",
        "# most occuring genre\n",
        "# list all the superheroes\n",
        "\n",
        "# Dataset\n",
        "\n",
        "The dataset has 9 columns with 9.84k rows of movie data. The task that is to be performed for the dataset will consist of two different pipelines which will compare the best strategy for the given task to be performed on the movie dataset. The main problem solving task at hand will to find out what the  superhero names are using the statistics available in the dataset. An NLP system is required for this objective to find and narrow down the dataset to pin point the superheros that occur in the dataset. By implementing an NLP system there can be strong analysis on the dataset to find the correct criteria.\n",
        "\n",
        "** Debating whether to use this: Merchán-Rivera, P. (2025). movies-dataset [Dataset]. Hugging Face. https://huggingface.co/datasets/Pablinho/movies-dataset.\n",
        "\n",
        "Reference: https://colab.research.google.com/drive/1MbiTzuPPCKrj9BOu_Nj4gcsxxYKwLg0a?usp=sharing#scrollTo=rubk2y4rGx7y\n"
      ],
      "metadata": {
        "id": "gibHCW4NBSdD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files #import the files package from google.colab framework to be able to upload files\n",
        "\n",
        "from sklearn.pipeline import Pipeline #let's import the pipeline functionality\n",
        "from sklearn.feature_extraction.text import CountVectorizer #and we will import a simple pre-processing method\n",
        "from sklearn.feature_extraction.text import TfidfTransformer #and a representation learner\n",
        "from sklearn.neighbors import KNeighborsClassifier #and a simple classifier model\n",
        "from sklearn.model_selection import StratifiedKFold #cross fold is sometimes called k-fold. Calling the stratified version ensures that classes have equal representation across folds\n",
        "from sklearn.metrics import accuracy_score #import an accuracy metric to tell us how well the model is doing\n",
        "\n",
        "import numpy as np\n",
        "np.random.seed(1337)\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"hf://datasets/Pablinho/movies-dataset/9000plus.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Ypu2LZD3FOP",
        "outputId": "166dfb95-d5d0-4d2c-8ee6-9b6e23128869"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Used ai to fix gathering dataset not being found. [1]\n",
        "\n",
        "x = df['Title'].astype(str).values\n",
        "y = df['Genre'].astype(str).values\n",
        "\n",
        "text_clf = Pipeline([ #the pipeline object allows us to organise a series of functions which will be applied to our text data as though they were a single function\n",
        "  ('prep', CountVectorizer()), #we will use a simple count vectorizer for our pre-processing (which cheats a little by combining numerous pre-processing steps)\n",
        "  ('rep', TfidfTransformer()), #and a representation learning method using tf-idf\n",
        "  ('mod', KNeighborsClassifier()), #and a simple kNN classifier\n",
        "  ])\n",
        "\n",
        "acc_score = [] #create a list to store the accuracy values\n",
        "\n",
        "kf = StratifiedKFold(n_splits=5) #we instantiate the kfold instance, and set the number of folds to 5\n",
        "for train, test in kf.split(x,y): #we use a for loop to iterate through each fold using the train and test indexes from the dataset\n",
        "\n",
        "  x_train, x_test, y_train, y_test = x[train], x[test], y[train], y[test] #things can get a bit weird when inputting indexes to functions, so lets save them as variables\n",
        "  #print(train)\n",
        "  #print(test) #this will print the train and test indexes respectively, if you want to be sure they do not overlap\n",
        "\n",
        "  text_clf.fit(x_train, y_train) #we then only fit the training data (note that we oapply the text_clf pipeline object, rather than having to go through each function separately)\n",
        "  predictions = text_clf.predict(x_test) #and can predict on the test data (similar to above, we can predict using the pipeline directly)\n",
        "  acc = accuracy_score(predictions, y_test) #we use the accuracy score we imported to give an idea how well the model is doing\n",
        "  acc_score.append(acc) #we can append it to our list\n",
        "\n",
        "print(\"Accuracy:\", np.mean(acc_score))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ku4DdlyY8T59",
        "outputId": "41257064-8647-4790-88d2-26cf128c4b69"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/model_selection/_split.py:805: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.02886933177923543\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(x_train[0]) #first let's double check our input data is just a simple sentence"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ONJfHmHxPzrH",
        "outputId": "ffc29c66-b300-42d3-f109-8b9d12af7e54"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spider-Man: No Way Home\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk #import the natural language toolkit\n",
        "\n",
        "nltk.download('punkt') #download the package in nltk which supports tokenization\n",
        "nltk.download('stopwords') #download the nltk package for stopwords\n",
        "\n",
        "from nltk.tokenize import word_tokenize #import the tokenize package\n",
        "from nltk.corpus import stopwords #import the package from the corpus\n",
        "from nltk.stem.snowball import SnowballStemmer #import the snowball stemmer (also known as Porter2)\n",
        "\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "class pre_process(BaseEstimator, TransformerMixin):\n",
        "\n",
        "    def __init__(self):\n",
        "      return None #we do not need any parameters to instantiate this class\n",
        "\n",
        "    def fit(self, X, y=None): #both fit and transform expect the data instances and labels to be called - we do not use the labels, so set y=None\n",
        "        return self #as explained above, we will not use the fit method\n",
        "\n",
        "    def transform(self, X, y=None):\n",
        "      prep_text = []\n",
        "      for x in X: #for each sentence in the whole dataset\n",
        "            token_text = word_tokenize(x) #tokenize the document\n",
        "            normd_text = [token.lower() for token in token_text if token.isalpha()] #list compression to apply some simple cleaning (lower case casting and punctuation removal) to tokenized terms\n",
        "\n",
        "            swr_text = [token for token in normd_text if token not in stopwords.words('english')] #list compression to remove any stopwords from our list\n",
        "\n",
        "            stemmer = SnowballStemmer(\"english\") #specify we are using the English stemming rules, as other languages are present in toolkit\n",
        "            prep_text += [[stemmer.stem(word) for word in swr_text]] #list compression for applying the stemmer\n",
        "\n",
        "      prep_sentences = [\" \".join(sentence) for sentence in prep_text] #we join the sentences back together to ensure compatibility with CountVec, which is doing some of it's own prep\n",
        "      return prep_sentences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ubufY8rF6i1t",
        "outputId": "67dc2f7b-db55-4a92-f6ee-6a94ba79e369"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Representation Learning\n",
        "\n",
        "The reprensentation learning method used will be the Word-2-Vector(Word2Vec). Word2Vec is a technique which is in Natural language processing. The technique permits words to have reprentation as vectors which are in a continuous vector space. The representation learned for every word will be \"prediciting a target term using its context(CBOW)\"(Martin, 2025, slide 49). Continous Bag of Words (Continuos Bag of Words) model will be used to predict a word that is given words that is located in a window. Input layer will have the context words while output layer will have a current word. Hidden layer has dimensions that will show a current word that is located on the output layer.(Kadam, 2025)\n",
        "\n",
        "\n",
        "Kyle, M.(2025)'Natural Language Processing'[PowerPoint presentation]. CE4145: CE4145 & CEM300 W5 - Text Pre-Processing & Representation Learning. Available at: https://campusmoodle.rgu.ac.uk/mod/resource/view.php?id=5411825 (Accessed: 18 October 2025).\n",
        "\n",
        "Sumedh, K.(2025) Word Embedding using Word2Vec. Available at: https://www.geeksforgeeks.org/python/python-word-embedding-using-word2vec/ (Accessed: 22 October 2025).\n",
        "\n",
        "Manan S.(2022) A Dummy's Guide to Word2Vec. Available at: https://medium.com/@manansuri/a-dummys-guide-to-word2vec-456444f3c673 (Accessed: 01 November 2025)."
      ],
      "metadata": {
        "id": "I4fSC1u8Ebqu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim"
      ],
      "metadata": {
        "id": "VElZcAX79t9i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73f22faa-0ad6-4f11-b074-c65dbbbdd3d8"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gensim\n",
            "  Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.16.3)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.4.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim) (2.0.0)\n",
            "Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (27.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.9/27.9 MB\u001b[0m \u001b[31m63.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: gensim\n",
            "Successfully installed gensim-4.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec #first we import word2vec, then a few familiar imports"
      ],
      "metadata": {
        "id": "Ag4oC9089X-H"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assigning string values\n",
        "texts = df['Overview'].astype(str).values"
      ],
      "metadata": {
        "id": "aWhhjsxzanou"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "wX4O7e_OXDQ7"
      },
      "outputs": [],
      "source": [
        "sentences = [line.split() for line in texts]\n",
        "\n",
        "w2v = Word2Vec(sentences, window=5, workers=4)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(sentences[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FLbuQGZSMjCk",
        "outputId": "81fc3cd0-8994-489a-e5ed-117c1ed1249b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Peter', 'Parker', 'is', 'unmasked', 'and', 'no', 'longer', 'able', 'to', 'separate', 'his', 'normal', 'life', 'from', 'the', 'high-stakes', 'of', 'being', 'a', 'super-hero.', 'When', 'he', 'asks', 'for', 'help', 'from', 'Doctor', 'Strange', 'the', 'stakes', 'become', 'even', 'more', 'dangerous,', 'forcing', 'him', 'to', 'discover', 'what', 'it', 'truly', 'means', 'to', 'be', 'Spider-Man.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Describe the theory behind the\n",
        "algorithms to be applied. **bold text**\n",
        "\n",
        "# Algorithms"
      ],
      "metadata": {
        "id": "b34dhQrEIoJM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multi-Layer Perceptron(MLP) is able to learn a non - linear function that is used to seperate data. All the layers in a multi layer perceptron are closely connected. Multiple hidden layers are also possible. Multilayer perceptrons have an input layer, hidden layer, and an output layer. For the input layer there needs to be a numerical vector. The number of indexes in a vector will be the size of input layer. Hidden layer is used to connect input and output layer by using weights & biases. There is an activation function in the hidden layer but it is usually different on each hidden layer. The output layer is dependant on the final activation function but normally softmax is used in classification. The ouput layer consists of a vector of probabilities that have the same length of the number of classes.\n",
        "\n",
        "Named Entity recognition(NER) works by grabbing the list of named entities that are in a document. The algorithm then labels the named entities into categories which consist of \"person names, organizations, locations, medical codes, time expressions, quantities, monetary values, and more.\"(Awan,2023). NER works by connecting unstructured and structured data. This allows a computer to go through lots of data and grab the necessary data that is highly valued.\n",
        "\n",
        "\n",
        "Abid A.(2023) What is Named Entity Recognition (NER)? Methods, Use Cases, and Challenges. Available at: https://www.datacamp.com/blog/what-is-named-entity-recognition-ner\n",
        "(Accessed: 01 November 2025).\n"
      ],
      "metadata": {
        "id": "IWbMMkS1mrUE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy\n",
        "!pip install nltk\n",
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9k_FsgZoI_fW",
        "outputId": "2c584255-8226-46f3-9c57-f3dedffd2fb7"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.12/dist-packages (3.8.7)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.0.13)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.11)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.0.10)\n",
            "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (8.3.6)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.12/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.5.1)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (0.20.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.32.4)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.12/dist-packages (from spacy) (2.11.10)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.1.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from spacy) (75.2.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (25.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.12/dist-packages (from spacy) (3.5.0)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.12/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.15.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2025.10.5)\n",
            "Requirement already satisfied: blis<1.4.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.3.0)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.3.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.12/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.23.0)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.12/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.4.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->spacy) (3.0.3)\n",
            "Requirement already satisfied: marisa-trie>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.3.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.2)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (2.0.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n",
            "Collecting en-core-web-sm==3.8.0\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m51.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df) #if we print the dataset, we will see it appears as a dictionary\n",
        "print(df['Overview'][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DKEMTi-iRnIl",
        "outputId": "242aedca-427a-49b1-aad9-b56173e2677b"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     Release_Date                                 Title  \\\n",
            "0      2021-12-15               Spider-Man: No Way Home   \n",
            "1      2022-03-01                            The Batman   \n",
            "2      2022-02-25                               No Exit   \n",
            "3      2021-11-24                               Encanto   \n",
            "4      2021-12-22                        The King's Man   \n",
            "...           ...                                   ...   \n",
            "9832   1973-10-15                              Badlands   \n",
            "9833   2020-10-01                      Violent Delights   \n",
            "9834   2016-05-06                          The Offering   \n",
            "9835   2021-03-31  The United States vs. Billie Holiday   \n",
            "9836   1984-09-23                               Threads   \n",
            "\n",
            "                                               Overview  Popularity  \\\n",
            "0     Peter Parker is unmasked and no longer able to...    5083.954   \n",
            "1     In his second year of fighting crime, Batman u...    3827.658   \n",
            "2     Stranded at a rest stop in the mountains durin...    2618.087   \n",
            "3     The tale of an extraordinary family, the Madri...    2402.201   \n",
            "4     As a collection of history's worst tyrants and...    1895.511   \n",
            "...                                                 ...         ...   \n",
            "9832  A dramatization of the Starkweather-Fugate kil...      13.357   \n",
            "9833  A female vampire falls in love with a man she ...      13.356   \n",
            "9834  When young and successful reporter Jamie finds...      13.355   \n",
            "9835  Billie Holiday spent much of her career being ...      13.354   \n",
            "9836  Documentary style account of a nuclear holocau...      13.354   \n",
            "\n",
            "     Vote_Count Vote_Average Original_Language  \\\n",
            "0          8940          8.3                en   \n",
            "1          1151          8.1                en   \n",
            "2           122          6.3                en   \n",
            "3          5076          7.7                en   \n",
            "4          1793          7.0                en   \n",
            "...         ...          ...               ...   \n",
            "9832        896          7.6                en   \n",
            "9833          8          3.5                es   \n",
            "9834         94          5.0                en   \n",
            "9835        152          6.7                en   \n",
            "9836        186          7.8                en   \n",
            "\n",
            "                                   Genre  \\\n",
            "0     Action, Adventure, Science Fiction   \n",
            "1               Crime, Mystery, Thriller   \n",
            "2                               Thriller   \n",
            "3     Animation, Comedy, Family, Fantasy   \n",
            "4       Action, Adventure, Thriller, War   \n",
            "...                                  ...   \n",
            "9832                        Drama, Crime   \n",
            "9833                              Horror   \n",
            "9834           Mystery, Thriller, Horror   \n",
            "9835               Music, Drama, History   \n",
            "9836         War, Drama, Science Fiction   \n",
            "\n",
            "                                             Poster_Url  \n",
            "0     https://image.tmdb.org/t/p/original/1g0dhYtq4i...  \n",
            "1     https://image.tmdb.org/t/p/original/74xTEgt7R3...  \n",
            "2     https://image.tmdb.org/t/p/original/vDHsLnOWKl...  \n",
            "3     https://image.tmdb.org/t/p/original/4j0PNHkMr5...  \n",
            "4     https://image.tmdb.org/t/p/original/aq4Pwv5Xeu...  \n",
            "...                                                 ...  \n",
            "9832  https://image.tmdb.org/t/p/original/z81rBzHNgi...  \n",
            "9833  https://image.tmdb.org/t/p/original/4b6HY7rud6...  \n",
            "9834  https://image.tmdb.org/t/p/original/h4uMM1wOhz...  \n",
            "9835  https://image.tmdb.org/t/p/original/vEzkxuE2sJ...  \n",
            "9836  https://image.tmdb.org/t/p/original/lBhU4U9Eeh...  \n",
            "\n",
            "[9828 rows x 9 columns]\n",
            "Peter Parker is unmasked and no longer able to separate his normal life from the high-stakes of being a super-hero. When he asks for help from Doctor Strange the stakes become even more dangerous, forcing him to discover what it truly means to be Spider-Man.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import spacy\n",
        "\n",
        "# Load spaCy English model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# View columns\n",
        "print(df.columns)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OaIqO_Oecyy3",
        "outputId": "13d4a98e-215e-4cd7-cfdc-4ca37e7f98a9"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['Release_Date', 'Title', 'Overview', 'Popularity', 'Vote_Count',\n",
            "       'Vote_Average', 'Original_Language', 'Genre', 'Poster_Url'],\n",
            "      dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#Asked AI for support with gathering the entity names [2]\n",
        "\n",
        "# Drop rows with missing values in 'Overview' column before processing\n",
        "df.dropna(subset=['Title'], inplace=True)\n",
        "\n",
        "# Iterate through the 'Overview' column and apply nlp to each overview\n",
        "for title in df['Title']:\n",
        "    doc = nlp(title)\n",
        "    for ent in doc.ents:\n",
        "        print(ent.text, ent.start_char, ent.end_char, ent.label_)"
      ],
      "metadata": {
        "id": "02270EFdSae6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy import displacy\n",
        "displacy.render(doc, style=\"ent\")"
      ],
      "metadata": {
        "id": "LisByfX3P9sa",
        "outputId": "645ddbec-301e-408a-cec8-270dae77a207",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"59e6bd1aae664568b9803f1d783b07c6-0\" class=\"displacy\" width=\"225\" height=\"137.0\" direction=\"ltr\" style=\"max-width: none; height: 137.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"47.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">Threads</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">NOUN</tspan>\n",
              "</text>\n",
              "</svg></span>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "entities = [(ent.text, ent.label_, ent.lemma_) for ent in doc.ents]\n",
        "df = pd.DataFrame(entities, columns=['text', 'type', 'lemma'])\n",
        "print(df)"
      ],
      "metadata": {
        "id": "CJ_gY4xgQC4w",
        "outputId": "11af6402-e3ed-43ff-921d-abfd712378f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'doc' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2664264759.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mentities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemma_\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ment\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ments\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentities\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'type'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'lemma'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'doc' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from spacy.training.example import Example #import the data structure required as input\n",
        "\n",
        "train_data = [\n",
        "    (\"Tom 0 3\", {\"entities\":[(0,4, \"PERSON\")]}), #create some training data, which is annotated text\n",
        "    (\"This is an example of Tom 0 3\", {\"entities\":[(13,16, \"Algorithms\")]}) #note we state the start and end character and a new label\n",
        "    ]\n",
        "\n",
        "ner=df.get_pipe(\"ner\") #we then extract specifically the ner component of the pipeline\n",
        "\n",
        "for n in range (100): #training will take multiple iterations\n",
        "  for text, annotations in train_data:\n",
        "    for ent in annotations.get(\"entities\"): #we extract the entity component\n",
        "      ner.add_label(ent[2]) #and add the label to our list of possible entities\n",
        "    # create Example\n",
        "    doc = nlp.make_doc(text)\n",
        "    example = Example.from_dict(doc, annotations) #we then put the information into an Example data structure\n",
        "    # Update the model\n",
        "    nlp.update([example]) #and update the model"
      ],
      "metadata": {
        "id": "UQsGxqbOQHLz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "b41f296b-237c-4487-92d2-43e243f8e863"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'df' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3634927776.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m     ]\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_pipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ner\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#we then extract specifically the ner component of the pipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m#training will take multiple iterations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reference to add: https://www.geeksforgeeks.org/nlp/named-entity-recognition/"
      ],
      "metadata": {
        "id": "3EfqAcFiP2pd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# MLP Output\n",
        "\n"
      ],
      "metadata": {
        "id": "a_rhqQapPKee"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras #we will only be using tensorflow as a backend, so we will import keras via their implementation\n",
        "from keras import layers #we will use the readable keras implementation of the dense layers\n",
        "from keras import activations #and the keras activation functions\n"
      ],
      "metadata": {
        "id": "Z4HUgTU0g21H"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mlp = keras.Sequential( #first we create a model and specify we are using the Sequential configuration within Keras\n",
        "    [\n",
        "    keras.Input(shape=(95,)), #Next we specify the size of our input layer. For our dataset we have 95 features. If the data were multiple dimensions, we would add more after the comma.\n",
        "    keras.layers.Dense(64, activations.relu), #Then we have our hidden layer. We have used 64 neurons, with a relu activation (consistent with above)\n",
        "    keras.layers.Dense(2, activations.softmax) #We then send to our output layer (which has 2 neurons, one for each class) and a softmax activation function\n",
        "    ]\n",
        ")\n",
        "\n",
        "mlp.summary() #we can view a summary of the model, including the number of trainable parameters (i.e. weights + biases) and the storage size."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196
        },
        "id": "R7M7kqfeg5Aj",
        "outputId": "00f94bc5-0187-496b-851a-daed0f8172f1"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_3\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_3\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ dense_4 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m6,144\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_5 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)              │           \u001b[38;5;34m130\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ dense_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">6,144</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">130</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m6,274\u001b[0m (24.51 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">6,274</span> (24.51 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m6,274\u001b[0m (24.51 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">6,274</span> (24.51 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.33, random_state=1337) #let's use a static split for clarity\n",
        "\n",
        "num_classes = 2 #we need to identify the number of output classes - here we have 2 - bankrupt or not.\n",
        "\n",
        "#Then we convert class vectors to one hot vectors - i.e. vectors of the size of number of classes, with a 1 in the class index and a 0 in every other class index\n",
        "y_traink = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_testk = keras.utils.to_categorical(y_test, num_classes)\n",
        "\n",
        "batch_size = 64 #the batch size is the number of examples the model will view before summing and backpropagating the loss\n",
        "epochs = 10 #epochs are the number of training iterations examining the full training set\n",
        "loss = \"categorical_crossentropy\" #loss is how we measure the error. Categorical crossentropy compares the probability for a prediction with it's ground truth (i.e. the one-hot vector we just created above)\n",
        "optimizer=\"sgd\" #optimizer updates the parameters on the backward pass. SGD is a type of gradient descent to do this.\n",
        "\n",
        "\n",
        "mlp.compile(loss=loss, optimizer=optimizer, metrics=[\"accuracy\", \"f1_score\"]) #We can set the model to compile.\n",
        "mlp.fit(x_train, y_traink, batch_size=batch_size, epochs=epochs) #Finally, we train the model\n"
      ],
      "metadata": {
        "id": "Y03VqD--jXQf",
        "outputId": "5af13e4a-4d0c-460f-8f1a-804bd7eb5fb5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        }
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "invalid literal for int() with base 10: 'Horror, Comedy'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1309602744.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#Then we convert class vectors to one hot vectors - i.e. vectors of the size of number of classes, with a 1 in the class index and a 0 in every other class index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0my_traink\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_categorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0my_testk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_categorical\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/keras/src/utils/numerical_utils.py\u001b[0m in \u001b[0;36mto_categorical\u001b[0;34m(x, num_classes)\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnewshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mone_hot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"int64\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m     \u001b[0minput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: 'Horror, Comedy'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#We can then test our model on the test set\n",
        "score = mlp.evaluate(x_test, y_testk) #keras has an inbuilt evaluation function that will produce predictions and scores for a test set."
      ],
      "metadata": {
        "id": "ZP6okvugjiIN",
        "outputId": "cecc610d-9d1e-4fd0-9bad-7c1224597b79",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        }
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'y_testk' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3250666483.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#We can then test our model on the test set\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mscore\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_testk\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#keras has an inbuilt evaluation function that will produce predictions and scores for a test set.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'y_testk' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Describe the evaluation process, and\n",
        "analyse the results. **bold text**\n",
        "\n",
        "# Evaluation"
      ],
      "metadata": {
        "id": "TYIwd6xdI8hp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "scores = [0.4826, 0.4718, 0.4579, 0.4579, 0.4764]\n",
        "alg_scores = [f1_scores] + [scores]\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.set_title('Algorithm Performance')\n",
        "ax.boxplot(alg_scores)\n",
        "ax.set_xticklabels([\"SKLearn\", \"Keras\"], rotation='vertical')"
      ],
      "metadata": {
        "id": "UvDxKXWRgsDa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}